{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FE545 - Homework #4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Author**: Sid Bhatia\n",
    "\n",
    "**Date**: April 6th, 2023\n",
    "\n",
    "**Pledge**: I pledge my honor that I have abided by the Stevens Honor System.\n",
    "\n",
    "**Professor**: Steve Yang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pricing a derivative security entails calculating the *expected discounted value* of its *payoff*. This reduces, in principle, to a problem of numerical integration; but in practice this calculation is often difficult for high-dimensional pricing problems. \n",
    "\n",
    "**Broadie** and **Glasserman** (1997)  proposed the method of the simulated tree to price American options, which can derive the upper and lower bounds for American options. This combination makes it possible to measure and control errors as the computational effort increases. \n",
    "\n",
    "The main drawback of the random tree method is that its computational requirements grow exponentially in the number of exercise dates $m$, so the method is applicable only when $m$ is small. Nevertheless, for problems with small $m$, it is very effective, and it also serves to illustrate a theme of managing sources of high and low bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\tilde{h_i}$ denote the payoff function for exercise at $t_i$ which we now allow to depend on $i$. Let $\\tilde{V}_i(x)$ denote the value of the option at $t_i$ given $X_i = x_i$ (the option has not been exercised). We are ultimately interested in $\\tilde{V}_0(X_0).$ This value is determined recursively as follows:\n",
    "\n",
    "$$\\tilde{V}_m(x) = \\tilde{h}_m(x) \\tag{1}$$\n",
    "\n",
    "$$\\tilde{V}_{i-1}(x) = \\max\\{\\tilde{h}_{i-1}(x), \\; \\mathbb{E}^Q[D_{i-1, i}(X_i) \\cdot \\tilde{V}_i(X_i) \\mid X_{i-1} = x]\\} \\tag{2}$$\n",
    "\n",
    "For $i = 1, \\dots, m$, we have introduced the notation $D_{i-1, i}(X_i)$ for the **discount factor** from $t_{i-1}$ to $t$. It states that the option value at expiration is given by the payoff function $\\tilde{h}_m$; and, at the $(i - 1)$-th exercise date, the option value is the **maximum** of the *immediate exercise value* and the *expected present value of continuing*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### High Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As its name suggests, the random tree method is based on simulating a tree of paths of the underlying **Markov Chain** $X_0, X_1, \\dots, X_m$. Fix a branching parameter $b \\geq 2$. From the initial state $X_0$, simulate $b$ independent successor states  $X_1^1, \\dots, X_1^B$ all having the **law** of $X_1$. For each $X_1^i$, simulate $b$ independent successors $X_1^{i1}, \\dots X_1^{ib}$ from the **conditional law** of $X_2$ given $X_1 = X_1^i$. From each $X_2^{i_1 i_2}$, generate $b$ successors $X_3^{i_1 i_2 1}, \\dots X_3^{i_1 i_2 b}$, and so on. We denote a *generic node* in the tree at time step $i$ by $X_i^{j_1 j_2 \\dots j_i}$. The superscript indicates that this node is reached by following the $j$-th branch out of $X_0$, the $j_2$-th branch out of the next node, and so on. \n",
    "\n",
    "Although it is not essential that the branching parameter remain fixed across time steps, this is a convenient simplification in discussing the method. From the random tree method we define high and low estimators at each node by backward induction.\n",
    "\n",
    "We use **formulation 2**. Thus, $\\hat{h}_i$ is the discounted payoff function at the $i$-th exercise date, and the discounted option value satisfies $\\hat{V}_m = \\hat{h}_m$,\n",
    "\n",
    "$$\n",
    "\\tilde{V}_i(x) = \\max{\\left(\\tilde{h}_i(x), \\mathbb{E}^Q[\\tilde{V}_{i+1}(X_{i+1}) \\mid X_i = x]\\right)}, \\; i = 1, \\dots, m - 1 \\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write $\\hat{V}_i^{j_i \\dots j_i}$ for the **value of the high estimator** at node $X_i^{j_i \\dots j_i}$. At the terminal nodes, we set\n",
    "\n",
    "$$\n",
    "\\hat{V}_i^{j_1 \\dots j_m} h_m (X_m^{j_1 \\dots j_m}). \\tag{4}\n",
    "$$\n",
    "\n",
    "Working backward, we then set\n",
    "\n",
    "$$\n",
    "\\hat{V}_i^{j_1 \\dots j_i} = \\max{\\left( h_i (X_i^{j_1 \\dots j_i}), \\; \\frac{1}{b} \\sum_{j=1}^b \\hat{v}_{i+1}^{j_1 \\dots j_i j}\\right)}. \\tag{5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Formulation 5** is based on successor nodes, so the estimator is unfairly peeking into the future in making its decision. To remove this source of bias, we need to separate the exercise decision from the value received upon continuation. A new estimator can be defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At all terminal nodes, set the estimator equal to the payoff at that node:\n",
    "\n",
    "$$\n",
    "\\hat{v}_m^{j_1 \\dots j_m} = h_m(X_m^{j_1 \\dots j_m}) \\tag{6}.\n",
    "$$\n",
    "\n",
    "At node $j_1, j_2, \\dots j_i$ at time step $i$, and for each $k = 1, \\dots, b$, set\n",
    "\n",
    "$$\n",
    "\\hat{v}_{ik}^{j_1 j_2 \\dots j_i} = \\begin{cases}\n",
    "\n",
    "h_i (X_i^{j_1 j_2 \\dots j_i}) & \\; \\text{if} \\; \\frac{1}{b} \\sum_{j = 1}^b \\hat{v}_{i + 1}^{j_1 j_2 \\dots j_i j} \\leq h_i (X_i^{j_1 j_2 \\dots j_i}); \\tag{7} \\\\\n",
    "\n",
    "\\hat{v}_{i + 1}^{j_1 j_2 \\dots j_i k} & \\; \\text{otherwise}\n",
    "\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Then set\n",
    "\n",
    "$$\n",
    "\\hat{v}_i^{j_1 \\dots j_i} = \\frac{1}{b} \\sum_{k=1}^b \\hat{v}_{i,k}^{j_1 \\dots j_i} \\tag{8}\n",
    "$$\n",
    "\n",
    "Working backward, we then set\n",
    "\n",
    "$$\n",
    "\\hat{V}_i^{j_1 \\dots j_i} = \\max{\\left( h_i(X_i^{j_1 \\dots j_1}), \\frac{1}{b} \\sum_{k=1}^b \\hat{V}_{i,k}^{j_1 \\dots j_i})\\right) } \\tag{9}\n",
    "$$\n",
    "\n",
    "The high estimator of the option price at the current time and state is $\\hat{v}_0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other words, the high estimator is simply the result of applying **ordinary dynamic programming** to the *random tree*, assigning equal weight to each branch. Its calculation is illustrated in Figure 1 with $h_i(x) = (x - 100)_+$.\n",
    "\n",
    "A simple induction argument demonstrates that the high estimator is **indeed biased high** at every node, in the sense that\n",
    "\n",
    "$$\n",
    "\\mathbf{E}^Q[\\hat{V}_i^{j_1 \\dots j_i} \\mid X_i^{j_1 \\dots j_i}] = \\mathbf{E}[V_i^{j_i \\dots j_i}] \\tag{10}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"./fe545-hw4-figures/fe545-hw4-figure0.png\" alt=\"Trinomial Tree\" style=\"width: 30%; height: auto; display: block; margin-left: auto; margin-right: auto;\">\n",
    "    <figcaption style=\"text-align: center; font-size: larger; margin-top: 10px\">Figure 1: High Estimator K = 100</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low estimator is defined as follows. At all terminal nodes, set the estimator equal to the payoff at that node:\n",
    "\n",
    "$$\n",
    "\\hat{v}_m^{j_1 j_2 \\dots j_m} = h_m (X_m^{j_1 j_2 \\dots j_m}) \\tag{7}\n",
    "$$\n",
    "\n",
    "At each node $j_1, j_2, \\dots, j_i$ at time step $i$, and for each $k = 1, \\dots b$, set\n",
    "\n",
    "$$\n",
    "\\hat{v}_{ik}^{j_1 j_2 \\dots j_i} = \\begin{cases}\n",
    "\n",
    "h_i (X_i^{j_1 j_2 \\dots j_i}) & \\; \\text{if} \\; \\frac{1}{b - 1} \\sum_{j=1; j \\neq k}^b \\hat{v}_{i+1}^{j_1 j_2 \\dots j_i j} \\leq h_i(X_i^{j_1 j_2 \\dots j_i}); \\tag{8}\n",
    "\n",
    "\\\\\n",
    "\n",
    "\\hat{v}_{i+1}^{j_1 j_2 \\dots j_i k} & \\; \\text{otherwise}\n",
    "\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "We then set\n",
    "\n",
    "$$\n",
    "\\hat{v}_i^{j_1 j_2 \\dots j_i} = \\frac{1}{b} \\sum_{k=1}^b \\hat{v}_{ik}^{j_1 j_2 \\dots j_i}. \\tag{9}\n",
    "$$\n",
    "\n",
    "The low estimator of the option price at the current time and state is $\\hat{v}_o$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<figure>\n",
    "    <img src=\"./fe545-hw4-figures/fe545-hw4-figure1.png\" alt=\"Trinomial Tree\" style=\"width: 30%; height: auto; display: block; margin-left: auto; margin-right: auto;\">\n",
    "    <figcaption style=\"text-align: center; font-size: larger; margin-top: 10px\">Figure 2: High Estimator K = 50</figcaption>\n",
    "</figure>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume a random tree for pricing American call option is given in Figure 1. Please use the random tree in Figure 1 to calculate the high and low estimate of the American call option price with a **strike price $K = 50$**. Please show your steps for both the high estimator and low estimator.\n",
    "\n",
    "Note: You do not need to write C++ program to get the price, but rather manually calculating the estimators following the algorithms introduced in class."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
